

def build_dict(only_data_files=False):
    import time
    from respond.models import TableFile
    from respond.models import SheetFile
    from respond.models import DataFile
    from respond.models import ReplyFile
    model_mapping = {
        'data_file': DataFile,
        'reply_file': ReplyFile,
        'sheet_file': SheetFile,
        'table_file': TableFile
    }
    if only_data_files:
        model_mapping = {'data_file': DataFile}
    # model_dicts = []
    # model_dicts = {}
    files_in_db = {}
    start_time_dict = time.time()
    for model_name, model in model_mapping.items():
        model_objects = model.objects.filter(file__isnull=False)
        if only_data_files:
            model_objects = model_objects.prefetch_related(
                'petition_file_control__file_control__data_group')
        print(f"Modelo: {model_name}, objetos: {model_objects.count()}")
        # for model_obj in model_objects[:2000]:
        for model_obj in model_objects:
            file_name = model_obj.file.name
            # file_name = model_obj.file
            if not file_name:
                continue
            new_path = None
            # if model_name != 'table_file':
            #     short_name = file_name.split("/")[-1]
            #     new_path = set_upload_path_BACK(model_obj, short_name)
            current_elem = {
                'id': model_obj.id,
                'model_name': model_name,
                'new_path': new_path,
                'file_name': file_name,
            }
            if only_data_files and model_obj.petition_file_control:
                current_elem['data_group_id'] = \
                    model_obj.petition_file_control.file_control.data_group_id
            if files_in_db.get(file_name):
                files_in_db[file_name].append(current_elem)
                continue
            files_in_db[file_name] = [current_elem]
            # model_dicts.append(args)
    # print("Archivos repetidos: ", len(repeated_files))
    end_time_dict = time.time()
    execution_time_dict = end_time_dict - start_time_dict
    print(f"Tiempo de ejecución creación diccionario: "
          f"{execution_time_dict} segundos")
    return files_in_db


def analyze_rep_files():
    from pprint import pprint
    from respond.models import DataFile
    files_in_db = build_dict(True)
    rep_files2 = [rep for rep in files_in_db.values() if len(rep) >= 2]
    print("Archivos repetidos:", len(rep_files2))
    three_or_more = [rep for rep in rep_files2 if len(rep) >= 3]
    print("Archivos repetidos 3 o más veces: ", len(three_or_more))
    counters = {}
    print_counter = 0
    duplicates_ids = []
    for file in rep_files2:
        models = [elem['model_name'] for elem in file]
        has_many_data_files = models.count('data_file') >= 2
        detailed = [
            elem for elem in file if elem['data_group_id'] == 'detailed']
        has_many_detailed = len(detailed) >= 2
        if has_many_data_files and has_many_detailed:
            # print("Data files: ", file)
            duplicates_ids.extend([elem['id'] for elem in file
                                   if elem['model_name'] == 'data_file'])
        models = tuple(sorted(models))
        # counters[models] = counters.get(models, 0) + 1
        if len(set(models)) == 1:
            model = f"{models[0]}_repeated_{len(file)}"
            counters[model] = counters.get(model, 0) + 1
        else:
            if print_counter < 10 and len(file) > 2:
                print_counter += 1
                print("Distintos: ", file)
            counters[models] = counters.get(models, 0) + 1
    print("Duplicates ids: ", len(duplicates_ids))
    DataFile.objects.filter(id__in=duplicates_ids).update(is_duplicated=True)
    print("Contadores:")
    pprint(counters)


def revert_duplicated():
    from respond.models import DataFile
    DataFile.objects.filter(is_duplicated=True).update(is_duplicated=None)


def get_bucket_files(limit=10000):
    import boto3
    import time
    from django.conf import settings
    # from task.models import FilePath

    bucket_name = getattr(settings, "AWS_STORAGE_BUCKET_NAME")
    aws_access_key_id = getattr(settings, "AWS_ACCESS_KEY_ID")
    aws_secret_access_key = getattr(settings, "AWS_SECRET_ACCESS_KEY")
    s3 = boto3.resource(
        's3', aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key)
    my_bucket = s3.Bucket(bucket_name)

    excluded_dirs = [
        "admin/", "aws_errors/", "cat_images/", "ckeditor/", "experiment/",
        "logos/", "mat_views/", "profile_images/", "rest_framework/"]

    # all_bucket_files = my_bucket.objects.all()
    files_in_db = build_dict()
    all_bucket_files = my_bucket.objects.filter(Prefix="data_files/")
    # all_bucket_files = my_bucket.objects.filter(
    #     Prefix="data_files/estatal/isem/202210")
    counter = 0
    count_after_exclusion = 0
    count_found = 0
    objs_to_save = []
    start_time_model = time.time()

    for bucket_obj in all_bucket_files:
        bucket_obj_key = bucket_obj.key.replace('data_files/', '')
        # print("key: ", key)
        if not any(excluded_dir in bucket_obj_key for excluded_dir in excluded_dirs):
            count_after_exclusion += 1
            # print("key: ", key)
            args = {'path_in_bucket': bucket_obj_key, 'size': bucket_obj.size}
            # for model_dict in model_dicts:
            #     model_obj = model_dict.get(bucket_obj_key)
            #     if model_obj is None:
            #         continue
            #     count_found += 1
            #     args['is_correct_path'] = bucket_obj_key == model_obj['new_path']
            #     args['path_to_file'] = model_obj['new_path']
            #     args[f"{model_obj['model_name']}_id"] = model_obj['id']
            #     # print("argumentos a guardar: ", args.items())
            #     break
            # created_obj = FilePath(**args)
            # objs_to_save.append(created_obj)
            if len(objs_to_save) >= 1000:
                # FilePath.objects.bulk_create(objs_to_save)
                objs_to_save.clear()
            counter += 1
        if counter >= limit:
            break

    # FilePath.objects.bulk_create(objs_to_save)
    print("Cuenta después exclusión: ", count_after_exclusion)
    print("Encontrados: ", count_found)
    end_time_model = time.time()
    execution_time = end_time_model - start_time_model
    print(f"Tiempo de ejecución creación modelo: {execution_time} segundos")


# def clean_file_path():
#     from task.models import FilePath
#     FilePath.objects.all().delete()


def dummy_change_path():
    import boto3
    from django.conf import settings

    bucket_name = getattr(settings, "AWS_STORAGE_BUCKET_NAME")
    aws_access_key_id = getattr(settings, "AWS_ACCESS_KEY_ID")
    aws_secret_access_key = getattr(settings, "AWS_SECRET_ACCESS_KEY")
    s3 = boto3.resource(
        's3', aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key)
    my_bucket = s3.Bucket(bucket_name)

    # No hay una función específica para mover. Las referencias sugieren
    # lo siguiente: Copiar el archivo a la nueva ruta con
    # el nombre incluido
    objs = my_bucket.objects.filter(Prefix="data_files/experiment")
    for obj in objs:
        if obj.key == "data_files/experiment/Prac04.pdf":
            my_bucket.copy(
                {'Bucket': bucket_name,
                 'Key': 'data_files/experiment/Prac04.pdf'},
                'static/Prac04.pdf')

    # y eliminar el original de la ruta donde se encontraba
    objs = my_bucket.objects.filter(Prefix="static/")
    for obj in objs:
        if obj.key == "static/Prac04.pdf":
            obj.delete()

    # obj = s3.Object(bucket_name, "data_files/experiment/Prac04.pdf")
    # print("Objeto a revisar: ", obj)

# test con la carpeta data_files/estatal/isem/202210 y el archivo específico
# correspondencia 926083.xlsx
# argumentos a guardar:  dict_items([
# ('path_in_bucket', 'estatal/isem/202210/correspondencia 926083.xlsx'),
# ('size', 19645),
# ('data_file', <DataFile: estatal/isem/202210/correspondencia 926083.xlsx Instituto de Salud del Estado de México (ISEM) -- 00872 - 683. Reporte de suministros>),
# ('path_fo_file', 'estatal/isem/00872/estatal/isem/202210/correspondencia 926083.xlsx'),
# ('is_correct_path', False)
# ])


# example_url = "estatal/isem/202210/correspondencia 926083.xlsx"
# only_file_name = example_url.split("/")[-1]

# list = [{'id': 1, 'model_name': 'data_file', 'file_name': 'estatal/isem/202210/correspondencia 926083.xlsx'},
#         {'id': 2, 'model_name': 'data_file', 'file_name': 'estatal/isem/202210/correspondencia 926083.xlsx'}]
# tuple_of_ids = tuple([elem['id'] for elem in list])
